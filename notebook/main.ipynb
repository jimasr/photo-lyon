{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining project: Discover and describe areas of interest an events from geo-located data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #1: Import Dataset and Librarie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of required libraries and dependencies\n",
    "# numeric calculations\n",
    "! pip install numpy==1.26.0 \n",
    "# data frames \n",
    "! pip install pandas==2.1.1 \n",
    "# machine learning algorithms \n",
    "! pip install scikit-learn==1.5.1 \n",
    "! pip install scipy==1.12.0\n",
    "# plotting \n",
    "! pip install plotly==5.24.1 \n",
    "! pip install matplotlib==3.8.0 \n",
    "! pip install seaborn==0.13.2 \n",
    "! pip install plotly-express==0.4.1 \n",
    "! pip install chart-studio==1.1.0 \n",
    "# web app library \n",
    "! pip install streamlit==1.37.1 \n",
    "# association rules\n",
    "! pip install mlxtend==0.23.3 \n",
    "! pip install folium==0.19.4\n",
    "! pip install geopy==2.4.1\n",
    "! pip install pyproj\n",
    "# text mining\n",
    "! pip install nltk==3.6.5\n",
    "! pip install wordcloud\n",
    "! pip install pillow==9.5.0\n",
    "! pip install osmnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas to deal with the data\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering,  DBSCAN  ,linkage_tree  \n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import folium\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy.spatial.distance import cdist\n",
    "from itertools import permutations\n",
    "import osmnx as ox\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV\n",
    "data = pd.read_csv(\"../data/dataset.csv\", sep=\",\", low_memory=False)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes 11 et 12 (indices 10 et 11 en pandas)\n",
    "columns_to_check = data.iloc[:, [11, 12]]\n",
    "\n",
    "# Parcourir chaque colonne sélectionnée\n",
    "for col in columns_to_check.columns:\n",
    "    print(f\"Analyse de la colonne: {col}\")\n",
    "    \n",
    "    # Trouver les types de données présents\n",
    "    types_present = data[col].map(type).value_counts()\n",
    "    print(\"Types présents dans cette colonne :\")\n",
    "    print(types_present)\n",
    "    \n",
    "    # Afficher l'utilisation de la mémoire pour cette colonne\n",
    "    memory_usage = data[col].memory_usage(deep=True)\n",
    "    print(f\"Utilisation de la mémoire pour cette colonne : {memory_usage} bytes\\n\")\n",
    "\n",
    "for col in columns_to_check.columns:\n",
    "    print(f\"Conversion de la colonne: {col}\")\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)  # Convertir en float, remplacer les erreurs par NaN\n",
    "\n",
    "    # Vérifier le résultat\n",
    "    print(f\"Colonne {col} après conversion:\")\n",
    "    print(data[col].head())  # Afficher les premières lignes pour valider\n",
    "    print(f\"Types après conversion: {data[col].map(type).value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columms\n",
    "data.columns = data.columns.str.strip()\n",
    "print(\"Colonnes après nettoyage :\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial: {len(data)}\")\n",
    "print(data.columns)\n",
    "columns_to_clean = [\n",
    "\t\t\t\t\t'lat', 'long', 'date_taken_minute', 'date_taken_hour', \n",
    "\t\t\t\t\t'date_taken_day', 'date_taken_month', 'date_taken_year', \n",
    "\t\t\t\t\t'date_upload_minute', 'date_upload_hour', 'date_upload_day', \n",
    "\t\t\t\t\t'date_upload_month', 'date_upload_year'\n",
    "\t\t\t\t\t]\n",
    "data_cleaned = data.dropna(subset=columns_to_clean)\n",
    "print(f\"After removing missing values: {len(data_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicates\n",
    "data_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates if necessary\n",
    "print(f\"Initial: {len(data_cleaned)}\")\n",
    "data_cleaned = data_cleaned.drop_duplicates(subset=['user','lat','long','title','date_taken_year','date_taken_month','date_taken_day'],keep='first')\n",
    "print(f\"After removing duplicates: {len(data_cleaned)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data with a non-null values at column 16 is sorted differently. We conclude that it is hard to determine which value belongs to which column since date_taken_minute, date_taken_month can be ambigous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
    "column_data = data_cleaned.dropna(subset=columns, how='all')\n",
    "\n",
    "print(column_data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows that have non-null values in columns 16, 17 and 18 are retracted from the dataset. These columns thus serve no purpose and are removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to check for non-null values\n",
    "columns_to_check = ['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
    "\n",
    "# Drop rows where any of the specified columns are not null\n",
    "print(f\"Initial: {len(data_cleaned)}\")\n",
    "data_cleaned = data_cleaned[data_cleaned[columns_to_check].isnull().all(axis=1)]\n",
    "print(f\"After removing non-null values: {len(data_cleaned)}\")\n",
    "\n",
    "# Drop the columns\n",
    "data_cleaned = data_cleaned.drop(columns=columns)\n",
    "\n",
    "print(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_csv(\"../data/data-cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe(exclude=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalisation Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=(45.764, 4.8357), zoom_start=12)\n",
    "\n",
    "# Sample a subset of the data (e.g., 500 points)\n",
    "sampled_data = data_cleaned.sample(n=1000, random_state=1)\n",
    "\n",
    "latitude_col = sampled_data['lat']\n",
    "longitude_col = sampled_data['long']\n",
    "\n",
    "for i in range(0, len(sampled_data)):\n",
    "\tfolium.Marker([latitude_col.iloc[i], longitude_col.iloc[i]]).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalisation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "m = folium.Map(location=(45.764, 4.8357), zoom_start=12)\n",
    "\n",
    "# Sample a subset of the data (e.g., 500 points)\n",
    "sampled_data = data_cleaned.sample(n=1000, random_state=1)\n",
    "\n",
    "latitude_col = sampled_data['lat']\n",
    "longitude_col = sampled_data['long']\n",
    "\n",
    "heat_data = [[row['lat'], row['long']] for index, row in sampled_data.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data necessary to create clusters here would be the latitude and longitude. We'll drop all other columns as it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to keep\n",
    "columns = ['lat', 'long']\n",
    "\n",
    "# Drop all columns except the specified ones\n",
    "df_clustering = data_cleaned[columns]\n",
    "print(df_clustering.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the longitude and latitude have comparable scales, we can use the StandardScaler to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_clustering)\n",
    "scaled_data_df = pd.DataFrame(data=scaled_data, columns=df_clustering.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaled_data_df['lat'], scaled_data_df['long'], alpha=0.5)\n",
    "plt.title('Scaled Latitude and Longitude')\n",
    "plt.xlabel('Scaled Latitude')\n",
    "plt.ylabel('Scaled Longitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the same pattern observed in the heatmap is reproduced with the scaled data. This is expected, as the data has been proportionally scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the optimal number of clusters using Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of k values to try\n",
    "k_values = range(1, 40)\n",
    "sum_square = []\n",
    "\n",
    "for k in k_values:\n",
    "\tkmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "\tkmeans.fit(scaled_data_df)\n",
    "\tsum_square.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_values, sum_square)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum square')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters can be interpreted to be between 6-10. We found that with 6-10 clusters, the granularity of places is not as well defined as the number of points are extremely vast and spreaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=70, init='k-means++')\n",
    "kmeans.fit(scaled_data_df)\n",
    "data_cleaned['cluster_kmeans'] = kmeans.labels_ \n",
    "print(data_cleaned[['cluster_kmeans', 'lat', 'long']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center map\n",
    "m = folium.Map(location=[data_cleaned['lat'].mean(), data_cleaned['long'].mean()], zoom_start=12)\n",
    "\n",
    "palette = sns.color_palette(\"hsv\", n_colors=data_cleaned['cluster_kmeans'].nunique())\n",
    "colors = [mcolors.rgb2hex(color) for color in palette]\n",
    "\n",
    "centroids = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "sampled_data = data_cleaned.sample(n=20000, random_state=1)\n",
    "cluster_counts = sampled_data['cluster_kmeans'].value_counts()\n",
    "\n",
    "valid_clusters = cluster_counts[cluster_counts >= 3].index.tolist()\n",
    "\n",
    "filtered_sampled_data = sampled_data[sampled_data['cluster_kmeans'].isin(valid_clusters)]\n",
    "cluster_id_sample = filtered_sampled_data['cluster_kmeans'].unique().tolist()\n",
    "\n",
    "print(cluster_id_sample) \n",
    "\n",
    "# Filter points based on distance to centroids\n",
    "filtered_points = filtered_sampled_data[filtered_sampled_data.apply(\n",
    "    lambda row: any(geodesic((row['lat'], row['long']), centroid).m <= 300 for centroid in centroids), axis=1)]\n",
    "\n",
    "# Map markers\n",
    "for i, row in filtered_points.iterrows():\n",
    "    point = (row['lat'], row['long'])\n",
    "    folium.CircleMarker(\n",
    "        location=point,\n",
    "        radius=1,\n",
    "        color=colors[row['cluster_kmeans']],\n",
    "        fill=True,\n",
    "        fill_color=colors[row['cluster_kmeans']]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Centroids and hulls\n",
    "for cluster_id in cluster_id_sample:\n",
    "    cluster_points = filtered_points[filtered_points['cluster_kmeans'] == cluster_id][['lat', 'long']].values\n",
    "    if len(cluster_points) >= 3:  # ConvexHull requires at least 3 points\n",
    "        try: \n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Skipping cluster {cluster_id} due to QhullError\")\n",
    "    centroid = centroids[cluster_id]\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Centroid {cluster_id}',\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"../html/kmeans_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_without_duplicates = df_clustering.drop_duplicates() # Drop duplicates\n",
    "print(f\"Initial: {len(df_clustering)}\")\n",
    "print(f\"After removing duplicates: {len(df_clustering_without_duplicates)}\")\n",
    "df_clustering_without_duplicates = df_clustering_without_duplicates.sample(n=10000, random_state=1) # Sample a subset of the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data_duplicates = scaler.fit_transform(df_clustering_without_duplicates)\n",
    "scaled_data_duplicates_df = pd.DataFrame(data=scaled_data_duplicates, columns=df_clustering.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaled_data_duplicates_df['lat'], scaled_data_duplicates_df['long'], alpha=0.5)\n",
    "plt.title('Scaled Latitude and Longitude')\n",
    "plt.xlabel('Scaled Latitude')\n",
    "plt.ylabel('Scaled Longitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que 'lat' et 'long' sont bien présentes dans le DataFrame\n",
    "if 'lat' not in df_clustering_without_duplicates.columns or 'long' not in df_clustering_without_duplicates.columns:\n",
    "    raise ValueError(\"Les colonnes 'lat' et 'long' doivent être présentes dans le DataFrame.\")\n",
    "\n",
    "# Si 'hierarchical_cluster' n'existe pas, on applique le clustering\n",
    "if 'hierarchical_cluster' not in df_clustering_without_duplicates.columns:\n",
    "    hierarchical_cluster = AgglomerativeClustering(n_clusters=20)  # Ajustez le nombre de clusters\n",
    "    df_clustering_without_duplicates['hierarchical_cluster'] = hierarchical_cluster.fit_predict(df_clustering_without_duplicates[['lat', 'long']])\n",
    "\n",
    "# Palette de couleurs\n",
    "palette = sns.color_palette(\"hsv\", n_colors=df_clustering_without_duplicates['hierarchical_cluster'].nunique())\n",
    "colors = [mcolors.rgb2hex(color) for color in palette]\n",
    "\n",
    "# Calculer les centroids manuellement pour chaque cluster\n",
    "centroids = df_clustering_without_duplicates.groupby('hierarchical_cluster')[['lat', 'long']].mean().values\n",
    "\n",
    "# Centrer la carte\n",
    "m = folium.Map(location=[df_clustering_without_duplicates['lat'].mean(), df_clustering_without_duplicates['long'].mean()], zoom_start=12)\n",
    "\n",
    "# Filtrer les points basés sur la distance aux centroids\n",
    "filtered_points = df_clustering_without_duplicates[df_clustering_without_duplicates.apply(\n",
    "    lambda row: any(geodesic((row['lat'], row['long']), centroid).m <= 200 for centroid in centroids), axis=1)]\n",
    "\n",
    "# Ajouter des points filtrés à la carte\n",
    "for i, row in filtered_points.iterrows():\n",
    "    point = (row['lat'], row['long'])\n",
    "    cluster_id = int(row['hierarchical_cluster'])  # Convertir en entier\n",
    "    folium.CircleMarker(\n",
    "        location=point,\n",
    "        radius=1,\n",
    "        color=colors[cluster_id],  # Utiliser 'hierarchical_cluster' pour la couleur\n",
    "        fill=True,\n",
    "        fill_color=colors[cluster_id]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Dessiner les Convex Hulls et les centroids\n",
    "for cluster_id in range(df_clustering_without_duplicates['hierarchical_cluster'].nunique()):\n",
    "    cluster_points = filtered_points[filtered_points['hierarchical_cluster'] == cluster_id][['lat', 'long']].values\n",
    "    if len(cluster_points) >= 3:  # ConvexHull nécessite au moins 3 points\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec le cluster {cluster_id}: {e}\")\n",
    "\n",
    "    centroid = centroids[cluster_id]\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Centroid {cluster_id}',\n",
    "    ).add_to(m)\n",
    "\n",
    "# Sauvegarder la carte\n",
    "m.save(\"../html/hierarchical_map.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "def plot_k_distance_graph(X, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    distances, _ = neigh.kneighbors(X)\n",
    "    distances = np.sort(distances[:, k-1])\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Points')\n",
    "    plt.ylabel(f'{k}-th nearest neighbor distance')\n",
    "    plt.title('K-distance Graph')\n",
    "\n",
    "    y_min, y_max = 0, 0.1\n",
    "    y_ticks = np.arange(y_min, y_max, 0.01)  # Création des ticks de 0.001 en 0.001\n",
    "    plt.yticks(y_ticks)\n",
    "    plt.grid(axis='y', which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "# Plot k-distance graph\n",
    "plot_k_distance_graph(scaled_data_df, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sélection des features pour le clustering\n",
    "features = data_cleaned[columns].drop_duplicates()\n",
    "\n",
    "# Normalisation des données complètes\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Appliquer DBSCAN\n",
    "dbscan = DBSCAN(eps=0.015, min_samples=50)\n",
    "clusters = dbscan.fit_predict(scaled_features)\n",
    "\n",
    "# Ajouter les clusters aux données\n",
    "features['cluster_dbscan'] = clusters\n",
    "#data_cleaned['cluster_dbscan'] = clusters\n",
    "# Afficher la distribution des clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_destination(number_of_points, features):\n",
    "    \"\"\"\n",
    "    Sélectionne aléatoirement un nombre donné de centroïdes à partir des clusters DBSCAN.\n",
    "    \n",
    "    :param number_of_points: Nombre de centroïdes à sélectionner.\n",
    "    :param features: DataFrame contenant les données avec la colonne 'cluster_dbscan'.\n",
    "    :return: Numpy array des centroïdes sélectionnés.\n",
    "    \"\"\"\n",
    "    clusters = [c for c in features['cluster_dbscan'].unique() if c != -1]  # Exclure les outliers (-1)\n",
    "    \n",
    "    if not clusters:\n",
    "        raise ValueError(\"Aucun cluster valide trouvé dans les données.\")\n",
    "\n",
    "    cluster_centroids = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        cluster_data = features[features['cluster_dbscan'] == cluster].drop(columns=['cluster_dbscan'])\n",
    "        centroid = cluster_data.mean().to_numpy()\n",
    "        cluster_centroids.append(centroid)\n",
    "\n",
    "    # Vérification du nombre de points demandés\n",
    "    max_points = len(cluster_centroids)\n",
    "    if number_of_points > max_points:\n",
    "        raise ValueError(f\"Le nombre de points demandés ({number_of_points}) est supérieur au nombre de clusters disponibles ({max_points}).\")\n",
    "\n",
    "    # Sélectionner aléatoirement le nombre de points demandés\n",
    "    selected_centroids = random.sample(cluster_centroids, number_of_points)\n",
    "\n",
    "    return np.array(selected_centroids)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "nbr = int(input(\"Enter the number of destinations you want to visit: \"))\n",
    "destination = gen_destination(nbr, features)\n",
    "print(destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsp_solver_brute_force(points):\n",
    "    \"\"\"\n",
    "    Résolution exacte du TSP avec une approche brute-force (permutation de tous les chemins possibles).\n",
    "    \n",
    "    :param points: Numpy array des coordonnées des points.\n",
    "    :return: Numpy array des coordonnées des points dans l'ordre optimal.\n",
    "    \"\"\"\n",
    "    num_points = len(points)\n",
    "    if num_points <= 1:\n",
    "        return points\n",
    "    \n",
    "    distance_matrix = cdist(points, points, metric='euclidean')  # Matrice des distances\n",
    "    \n",
    "    best_route = None\n",
    "    best_distance = float('inf')\n",
    "    \n",
    "    for perm in permutations(range(num_points)):  # Générer toutes les permutations possibles\n",
    "        distance = sum(distance_matrix[perm[i], perm[i + 1]] for i in range(num_points - 1))\n",
    "        if distance < best_distance:\n",
    "            best_distance = distance\n",
    "            best_route = perm\n",
    "    \n",
    "    return points[np.array(best_route, dtype=int)]  # Retourner les coordonnées dans l'ordre optimal\n",
    "\n",
    "# Application avec les données fournies\n",
    "\n",
    "optimized_coordinates_tsp = tsp_solver_brute_force(destination)\n",
    "print(optimized_coordinates_tsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une carte centrée sur la moyenne des coordonnées\n",
    "map_center = [optimized_coordinates_tsp[:, 0].mean(), optimized_coordinates_tsp[:, 1].mean()]\n",
    "\n",
    "m = folium.Map(location=[features['lat'].mean(), features['long'].mean()], zoom_start=12)\n",
    "\n",
    "# Filtrer les clusters valides (exclure le bruit: cluster -1)\n",
    "valid_clusters = features['cluster_dbscan'][features['cluster_dbscan'] != -1].unique()\n",
    "palette = sns.color_palette(\"hsv\", n_colors=len(valid_clusters))\n",
    "colors = {cluster: mcolors.rgb2hex(color) for cluster, color in zip(valid_clusters, palette)}\n",
    "\n",
    "# Sous-échantillonnage\n",
    "\n",
    "\n",
    "# Ajouter les points de clusters à la carte\n",
    "for i, row in features.iterrows():\n",
    "    cluster_id = row['cluster_dbscan']\n",
    "    if cluster_id == -1:  # Ignorer le bruit\n",
    "        continue\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lat'], row['long']),\n",
    "        radius=1,\n",
    "        color=colors[cluster_id],\n",
    "        fill=True,\n",
    "        fill_color=colors[cluster_id]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Calculer les barycentres des clusters pour les représenter comme \"centroïdes\"\n",
    "for cluster_id in valid_clusters:\n",
    "    cluster_points = features[features['cluster_dbscan'] == cluster_id][['lat', 'long']].values\n",
    "    \n",
    "    if len(cluster_points) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Barycentre du cluster\n",
    "    centroid = cluster_points.mean(axis=0)\n",
    "    \n",
    "    # Ajouter un marqueur pour le barycentre\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Cluster {cluster_id}',\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Dessiner l'enveloppe convexe si au moins 3 points\n",
    "    if len(cluster_points) >= 3:\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Skipping cluster {cluster_id} due to QhullError\")\n",
    "    \n",
    "graph = ox.graph_from_point(map_center, dist=5000, network_type='drive')\n",
    "\n",
    "# Trouver les nœuds les plus proches des points optimisés sur le réseau routier\n",
    "nearest_nodes = [ox.distance.nearest_nodes(graph, long, lat) for lat, long in optimized_coordinates_tsp]\n",
    "\n",
    "# Ajouter les points optimisés sur la carte\n",
    "for i, coord in enumerate(optimized_coordinates_tsp):\n",
    "    folium.Marker(\n",
    "        location=[coord[0], coord[1]],\n",
    "        popup=f'Point {i+1}',\n",
    "        icon=folium.Icon(color=\"green\", icon=\"info-sign\")\n",
    "    ).add_to(m)\n",
    "\n",
    "# Tracer l'itinéraire routier entre les points\n",
    "route_coords = []\n",
    "for i in range(len(nearest_nodes) - 1):\n",
    "    # Calculer le plus court chemin entre les nœuds\n",
    "    route = nx.shortest_path(graph, nearest_nodes[i], nearest_nodes[i+1], weight=\"length\")\n",
    "    # Obtenir les coordonnées des routes\n",
    "    route_coords.extend([(graph.nodes[node]['y'], graph.nodes[node]['x']) for node in route])\n",
    "\n",
    "# Ajouter la route optimisée sur la carte\n",
    "folium.PolyLine(\n",
    "    locations=route_coords,\n",
    "    color=\"red\",\n",
    "    weight=2.5,\n",
    "    opacity=1\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "# Sauvegarder la carte\n",
    "m.save(\"../html/dbscan_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Combine English and French stopwords.\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_fr = set(stopwords.words('french'))\n",
    "all_stopwords = stopwords_en.union(stopwords_fr)\n",
    "\n",
    "def clean_tags(tag_string, words_to_remove):\n",
    "    \"\"\"\n",
    "    Processes a comma-separated string of tags:\n",
    "      - Splits the string on commas.\n",
    "      - Removes any tag that is a stopword, contains a digit, or is in the custom removal list.\n",
    "      - Returns the cleaned tags as a comma-separated string.\n",
    "    \"\"\"\n",
    "    # Split the string on commas and remove extra whitespace.\n",
    "    raw_tags = [tag.strip() for tag in tag_string.split(',') if tag.strip()]\n",
    "    processed_tags = []\n",
    "    \n",
    "    for tag in raw_tags:\n",
    "        # Remove all internal whitespace\n",
    "        tag = re.sub(r'\\s+', '', tag)\n",
    "        \n",
    "        # Special handling: if the tag starts with \"uploaded:by=\",\n",
    "        # then remove that prefix and keep only the value (e.g., \"instagram\").\n",
    "        if tag.lower().startswith(\"uploaded:by=\"):\n",
    "            tag = tag[len(\"uploaded:by=\"):]\n",
    "        \n",
    "        processed_tags.append(tag)\n",
    "    \n",
    "    # Filter out tags that are stopwords, contain digits, or are in the additional removal list.\n",
    "    filtered_tags = [\n",
    "        tag for tag in processed_tags\n",
    "        if tag.lower() not in all_stopwords\n",
    "           and not re.search(r'\\d', tag)\n",
    "           and tag.lower() not in words_to_remove\n",
    "    ]\n",
    "    \n",
    "    return ','.join(filtered_tags)\n",
    "\n",
    "def clean_titles(title_string, words_to_remove):\n",
    "    \"\"\"\n",
    "    Processes a title string:\n",
    "      - Splits the title on whitespace.\n",
    "      - Removes any word that is a stopword, contains a digit, or is in the custom removal list.\n",
    "      - Returns the cleaned title as a space-separated string.\n",
    "    \"\"\"\n",
    "    words = [word.strip() for word in title_string.split() if word.strip()]\n",
    "    filtered_words = [\n",
    "        word for word in words\n",
    "        if word.lower() not in all_stopwords\n",
    "           and not re.search(r'\\d', word)\n",
    "           and word.lower() not in words_to_remove\n",
    "    ]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = {'iphone', 'iphonex', 'instagram', 'upload', 'rhone', \n",
    "                    'rhonealpes', 'lyon', 'france', 'rhônealpes'}\n",
    "\n",
    "\n",
    "if 'tags' in data_cleaned.columns:\n",
    "    data_cleaned['tags'] = data_cleaned['tags'].apply(\n",
    "        lambda x: clean_tags(x, words_to_remove) if isinstance(x, str) else x\n",
    "    )\n",
    "    # Remove rows where 'tags' becomes empty after cleaning.\n",
    "    data_cleaned = data_cleaned.dropna(subset=['tags'])\n",
    "    data_cleaned = data_cleaned[data_cleaned['tags'].str.strip() != '']\n",
    "\n",
    "if 'title' in data_cleaned.columns:\n",
    "    data_cleaned['title'] = data_cleaned['title'].apply(\n",
    "        lambda x: clean_tags(x, words_to_remove) if isinstance(x, str) else x\n",
    "    )\n",
    "    # Remove rows where 'titles' becomes empty after cleaning.\n",
    "    data_cleaned = data_cleaned.dropna(subset=['title'])\n",
    "    data_cleaned = data_cleaned[data_cleaned['title'].str.strip() != '']\n",
    "\n",
    "data_cleaned.to_csv(\"../data/text-mining.csv\", index=False)\n",
    "print(\"Data exported to 'text-mining.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_tags = ' '.join(data_cleaned['tags'].dropna())\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_tags)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Tags')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not that some text are not relevant for tagging purposes, ie. instagramapp, square, squareformat, etc.\n",
    "We could then proceed to remove some more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = {'square', 'squareformat', 'instagramapp', 'uploaded', 'iphoneography', 'europe', 'rhône'}\n",
    "\n",
    "\n",
    "if 'tags' in data_cleaned.columns:\n",
    "    data_cleaned['tags'] = data_cleaned['tags'].apply(\n",
    "        lambda x: clean_tags(x, words_to_remove) if isinstance(x, str) else x\n",
    "    )\n",
    "    # Remove rows where 'tags' becomes empty after cleaning.\n",
    "    data_cleaned = data_cleaned.dropna(subset=['tags'])\n",
    "    data_cleaned = data_cleaned[data_cleaned['tags'].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then revisualise again, these procedure can be done iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = ' '.join(data_cleaned['tags'].dropna())\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_tags)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Tags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency and inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_cluster_top_terms(cluster_id, data, cluster_col='cluster_kmeans', text_col='tags', top_n=10):\n",
    "    \"\"\"\n",
    "    Pour un cluster donné, cette fonction :\n",
    "      - Filtre les lignes du DataFrame pour ce cluster.\n",
    "      - Agrège le contenu textuel de la colonne spécifiée (par exemple, 'tags').\n",
    "      - Calcule la TF-IDF en utilisant les stopwords français.\n",
    "      - Retourne une liste des top_n termes triés par leur score TF-IDF.\n",
    "    \n",
    "    Parameters:\n",
    "        cluster_id : Identifiant du cluster.\n",
    "        data       : DataFrame contenant les données.\n",
    "        cluster_col: Nom de la colonne contenant les labels de cluster.\n",
    "        text_col   : Nom de la colonne contenant le texte.\n",
    "        top_n      : Nombre de termes à extraire.\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des top_n termes pour le cluster.\n",
    "    \"\"\"\n",
    "    cluster_data = data[data[cluster_col] == cluster_id]\n",
    "    if cluster_data.empty:\n",
    "        return []\n",
    "    \n",
    "    aggregated_text = \" \".join(cluster_data[text_col].astype(str).tolist())\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([aggregated_text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "    \n",
    "    if len(scores) < top_n:\n",
    "        top_n = len(scores)\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    top_terms = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    return top_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = sorted(data_cleaned['cluster_kmeans'].unique()) # For cluster kmeans\n",
    "\n",
    "for cluster_id in unique_clusters:\n",
    "    terms = get_cluster_top_terms(cluster_id, data_cleaned, cluster_col='cluster_kmeans', text_col='tags', top_n=10)\n",
    "    print(f\"Top terms for cluster {cluster_id}: {terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the events for Lyon in a dictionary, this dictionary can of course be expanded further but for the sake of simplicity, we've reduced to some of the major events in Lyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_in_lyon = {\n",
    "    \"Fête des Lumières\": {year: ((12, 8), (12, 11)) for year in range(2005, 2020)},\n",
    "    \"Nuits Sonores\": {\n",
    "        2005: ((5, 12), (5, 14)),\n",
    "        2006: ((5, 11), (5, 13)),\n",
    "        2007: ((5, 10), (5, 12)),\n",
    "        2008: ((5, 10), (5, 12)),\n",
    "        2009: ((5, 9),  (5, 11)),\n",
    "        2010: ((5, 9),  (5, 11)),\n",
    "        2011: ((5, 9),  (5, 11)),\n",
    "        2012: ((5, 9),  (5, 11)),\n",
    "        2013: ((5, 9),  (5, 11)),\n",
    "        2014: ((5, 9),  (5, 11)),\n",
    "        2015: ((5, 9),  (5, 11)),\n",
    "        2016: ((5, 9),  (5, 11)),\n",
    "        2017: ((5, 9),  (5, 11)),\n",
    "        2018: ((5, 9),  (5, 11)),\n",
    "        2019: ((5, 9),  (5, 11))\n",
    "    },\n",
    "    \"Fête de la Musique\": {year: ((6, 21), (6, 21)) for year in range(2005, 2020)},\n",
    "    \"Les Nuits de Fourvière\": {year: ((7, 1), (7, 15)) for year in range(2005, 2020)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first construct a datetime column from the year, month, and day columns by extracting the corresponding day-of-year and year (filtered to 2005–2019). \n",
    "\n",
    "Then, we aggregates the daily photo counts and applies a 7-day rolling mean to smooth out short-term fluctuations — this period is chosen because it captures a full week’s activity, which helps reveal underlying trends and recurring patterns while reducing noise.\n",
    "\n",
    "We then use peak detection (based on the 90th percentile threshold) to identify days with unusually high photo counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def event_from_day_of_year(day_of_year, reference_year):\n",
    "    \"\"\"\n",
    "    Convert a day-of-year (1-366) into a (month, day) tuple using the given reference year,\n",
    "    then check if it falls within any known event date ranges for that year.\n",
    "    \"\"\"\n",
    "    dt = datetime.datetime(reference_year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "    current_month, current_day = dt.month, dt.day\n",
    "    for event, event_dates in events_in_lyon.items():\n",
    "        # If event_dates is a dict, look up the range for the reference_year.\n",
    "        if isinstance(event_dates, dict):\n",
    "            if reference_year in event_dates:\n",
    "                (start_month, start_day), (end_month, end_day) = event_dates[reference_year]\n",
    "            else:\n",
    "                continue  # No date info for this event in this year.\n",
    "        else:\n",
    "            (start_month, start_day), (end_month, end_day) = event_dates\n",
    "        start_dt = datetime.datetime(reference_year, start_month, start_day)\n",
    "        end_dt = datetime.datetime(reference_year, end_month, end_day)\n",
    "        if start_dt <= dt <= end_dt:\n",
    "            return event\n",
    "    return None\n",
    "\n",
    "\n",
    "# Create a datetime column using the year, month, and day columns\n",
    "data_cleaned['date_taken'] = pd.to_datetime(\n",
    "    data_cleaned['date_taken_year'].astype(str) + '-' +\n",
    "    data_cleaned['date_taken_month'].astype(str) + '-' +\n",
    "    data_cleaned['date_taken_day'].astype(str),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Use the provided year column\n",
    "data_cleaned['year'] = data_cleaned['date_taken_year']\n",
    "data_cleaned['day_of_year'] = data_cleaned['date_taken'].dt.dayofyear\n",
    "\n",
    "years = sorted([y for y in data_cleaned['year'].dropna().unique() if 2005 <= y <= 2019]) # Limit to 2005-2019\n",
    "print(\"Years considered:\", years)\n",
    "\n",
    "n_years = len(years)\n",
    "fig, axes = plt.subplots(n_years, 1, figsize=(12, 4 * n_years), sharex=True)\n",
    "if n_years == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "peak_info = {}  # To store detected peaks and associated events for each year\n",
    "\n",
    "for i, yr in enumerate(years):\n",
    "    yearly_data = data_cleaned[data_cleaned['year'] == yr]\n",
    "    # Compute daily photo counts by day-of-year\n",
    "    daily_counts = yearly_data.groupby('day_of_year').size()\n",
    "    \n",
    "    # Apply a 7-day rolling mean to smooth the counts\n",
    "    daily_counts_smoothed = daily_counts.rolling(window=7, center=True).mean()\n",
    "    \n",
    "    # Work only with non-NaN values\n",
    "    smoothed = daily_counts_smoothed.dropna()\n",
    "    \n",
    "    if smoothed.empty:\n",
    "        print(f\"No data available for year {yr}\")\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"Photo Activity in {yr} (No Data)\")\n",
    "        continue\n",
    "    \n",
    "    # Detect peaks above the 90th percentile threshold\n",
    "    threshold = np.percentile(smoothed, 90)\n",
    "    peaks, properties = find_peaks(smoothed, height=threshold)\n",
    "    \n",
    "    # Use an empty list if no peaks are detected\n",
    "    if len(peaks) == 0:\n",
    "        detected_peak_days = []\n",
    "    else:\n",
    "        detected_peak_days = smoothed.index[peaks]\n",
    "    \n",
    "    # Associate each detected peak with a known event for that year\n",
    "    peak_events = {}\n",
    "    for peak_day in detected_peak_days:\n",
    "        ev = event_from_day_of_year(peak_day, int(yr))\n",
    "        peak_events[peak_day] = ev\n",
    "    peak_info[yr] = peak_events\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.plot(daily_counts.index, daily_counts, label='Daily Photo Counts', alpha=0.5)\n",
    "    ax.plot(smoothed.index, smoothed, label='7-Day Rolling Mean', color='red')\n",
    "    if len(peaks) > 0:\n",
    "        ax.scatter(smoothed.index[peaks], smoothed.iloc[peaks], \n",
    "                   color='green', marker='x', s=100, label='Detected Peaks')\n",
    "    ax.set_title(f\"Photo Activity in {yr}\")\n",
    "    ax.set_xlabel(\"Day of Year\")\n",
    "    ax.set_ylabel(\"Number of Photos\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These peaks can then be mapped to the dictionary defined earlier. An export of matching events can be found in output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "for yr in years:\n",
    "    printed_any = False\n",
    "    for peak_day, event in peak_info.get(yr, {}).items():\n",
    "        if event is not None:\n",
    "            dt = datetime.datetime(int(yr), 1, 1) + datetime.timedelta(days=peak_day - 1)\n",
    "            date_str = dt.strftime(\"%B %d\")  # e.g., \"December 09\"\n",
    "            print(f\"Year {yr} - Day {peak_day} ({date_str}) -> {event}\")\n",
    "            printed_any = True\n",
    "    if not printed_any:\n",
    "        print(f\"Year {yr}: No matching events\")\n",
    "\n",
    "output_dir = \"../output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "events_list = []\n",
    "for yr in years:\n",
    "    for peak_day, event in peak_info.get(yr, {}).items():\n",
    "        if event is not None:  # Only export if there's a matching event\n",
    "            dt = datetime.datetime(int(yr), 1, 1) + datetime.timedelta(days=peak_day - 1)\n",
    "            date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "            events_list.append({\n",
    "                \"year\": yr,\n",
    "                \"day_of_year\": peak_day,\n",
    "                \"date\": date_str,\n",
    "                \"event\": event\n",
    "            })\n",
    "\n",
    "events_df = pd.DataFrame(events_list)\n",
    "output_path = os.path.join(output_dir, \"events.csv\")\n",
    "events_df.to_csv(output_path, index=False)\n",
    "print(f\"Event data exported to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then conclude that the peaks of photo taken can be justified with events taking places at Lyon, especially Fête des Lumières."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
